---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<!-- {% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %} -->

## Tokenphormer: Structure-aware Multi-token Graph Transformer for Node Classification

Zijie Zhou, Zhaoqi Lu, Xuekai Wei, Rongqin Chen, Shenghui Zhang, Pak Lon Ip, and Leong Hou U. 
*Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2025*  
Volume 39, Number 12, Pages 13428â€“13436

[[Paper (AAAI)](https://ojs.aaai.org/index.php/AAAI/article/download/33466/35621)]
[[Github (Code)](https://github.com/Dodo-D-Caster/Tokenphormer)]

---

## Abstract

Graph Neural Networks (GNNs) are widely used in graph data mining tasks. Traditional GNNs follow a message passing scheme that can effectively utilize local and structural information. However, the phenomena of over-smoothing and over-squashing limit the receptive field in message passing processes. Graph Transformers were introduced to address these issues, achieving a global receptive field but suffering from the noise of irrelevant nodes and loss of structural information. Therefore, drawing inspiration from fine-grained token-based representation learning in Natural Language Processing (NLP), we propose the Structure-aware Multi-token Graph Transformer (Tokenphormer), which generates multiple tokens to effectively capture local and structural information and explore global information at different levels of granularity. Specifically, we first introduce the walk-token generated by mixed walks consisting of four walk types to explore the graph and capture structure and contextual information flexibly. To ensure local and global information coverage, we also introduce the SGPM-token (obtained through the Self-supervised Graph Pre-train Model, SGPM) and the hop-token, extending the length and density limit of the walk-token, respectively. Finally, these expressive tokens are fed into the Transformer model to learn node representations collaboratively. Experimental results demonstrate that the capability of the proposed Tokenphormer can achieve state-of-the-art performance on node classification tasks.

---

## BibTeX

```bibtex
@inproceedings{zhou2025tokenphormer,
  title={Tokenphormer: Structure-aware Multi-token Graph Transformer for Node Classification},
  author={Zhou, Zijie and Lu, Zhaoqi and Wei, Xuekai and Chen, Rongqin and Zhang, Shenghui and Ip, Pak Lon and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={12},
  pages={13428--13436},
  year={2025}
}
```
